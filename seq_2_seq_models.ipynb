{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fb265a-93a2-40ac-a66c-34f7765a1774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 16:12:59.566744: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-03 16:13:01.282353: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-03 16:13:01.282421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-03 16:13:01.465201: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-03 16:13:01.855837: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39190123-8e52-4fd4-aba7-1e8d430a00cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 16:14:13.411466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31134 MB memory:  -> device: 0, name: Tesla V100S-PCIE-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 12, 32)]             0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 64),                 24832     ['input_1[0][0]']             \n",
      "                              (None, 64),                                                         \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVecto  (None, 7, 64)                0         ['lstm[0][0]']                \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 7, 64)                33024     ['repeat_vector[0][0]',       \n",
      "                                                                     'lstm[0][1]',                \n",
      "                                                                     'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 7, 32)                2080      ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 59936 (234.12 KB)\n",
      "Trainable params: 59936 (234.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model\n",
    "def create_sequence_to_sequence_model(input_timesteps, input_features, output_timesteps):\n",
    "    # Input layer for the encoder\n",
    "    inputs = layers.Input(shape=(input_timesteps, input_features))\n",
    "\n",
    "    # Encoder LSTM\n",
    "    encoder = layers.LSTM(64, activation='relu', return_state=True, return_sequences=False)\n",
    "    encoder_outputs, state_h, state_c = encoder(inputs)\n",
    "\n",
    "    # Decoder LSTM: We now provide the encoder's state and initialize it with the encoder's final states\n",
    "    # We reshape the output of the encoder to make sure it is in the expected form for the decoder\n",
    "    decoder_input = layers.RepeatVector(output_timesteps)(encoder_outputs)\n",
    "\n",
    "    # Decoder LSTM, where the output sequence length is `output_timesteps` (7)\n",
    "    decoder_lstm = layers.LSTM(64, activation='relu', return_sequences=True)\n",
    "    decoder_outputs = decoder_lstm(decoder_input, initial_state=[state_h, state_c])\n",
    "\n",
    "    # Dense layer to predict the next 7 time periods (each with 32 features)\n",
    "    outputs = layers.Dense(input_features)(decoder_outputs)\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the input shape and output shape\n",
    "input_timesteps = 12  # 12 time periods\n",
    "input_features = 32   # 32 features per time period\n",
    "output_timesteps = 7  # Predict the next 7 time periods\n",
    "\n",
    "# Create the model\n",
    "model = create_sequence_to_sequence_model(input_timesteps, input_features, output_timesteps)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5f2bfa-45f4-438b-a7b0-d29d16d6845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 16:14:21.937476: I external/local_xla/xla/service/service.cc:168] XLA service 0x1503c8c8c860 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-06-03 16:14:21.937529: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100S-PCIE-32GB, Compute Capability 7.0\n",
      "2025-06-03 16:14:22.150741: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-06-03 16:14:22.826522: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748985263.235102 1707106 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 15s 28ms/step - loss: 0.0921\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0837\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0836\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0835\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0835\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0835\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0835\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0835\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0834\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0834\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0834\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0834\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0834\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0834\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0833\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0833\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0833\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 0.0833\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0833\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0832\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0832\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0831\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0831\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0831\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0831\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0831\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0831\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0831\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0831\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0831\n",
      "Epoch 59/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0831\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0830\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0830\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0830\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0830\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0830\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0830\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0830\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0830\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0829\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0829\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0829\n",
      "Epoch 71/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0829\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0829\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0829\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0829\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0828\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0828\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0828\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0828\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0828\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0828\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0828\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0827\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0827\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0827\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0828\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0827\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0827\n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0827\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0826\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0826\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0826\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0826\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0826\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0826\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0825\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.0825\n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0825\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0825\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0825\n",
      "Epoch 100/100\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1504e80ab010>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example training data (input and target sequences)\n",
    "import numpy as np\n",
    "\n",
    "# Random input data (batch_size, timesteps, features)\n",
    "X_train = np.random.random((10000, input_timesteps, input_features))\n",
    "\n",
    "# Random target data (batch_size, timesteps, features) - predicted next 7 time periods\n",
    "y_train = np.random.random((10000, output_timesteps, input_features))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a888b6-ce33-42d4-b74f-a044dd7ff325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 12, 32)]             0         []                            \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 64),                 24832     ['input_2[0][0]']             \n",
      "                              (None, 64),                                                         \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " repeat_vector_1 (RepeatVec  (None, 7, 64)                0         ['lstm_2[0][0]']              \n",
      " tor)                                                                                             \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               (None, 7, 64)                33024     ['repeat_vector_1[0][0]',     \n",
      "                                                                     'lstm_2[0][1]',              \n",
      "                                                                     'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 7, 32)                2080      ['lstm_3[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 59936 (234.12 KB)\n",
      "Trainable params: 59936 (234.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model\n",
    "def create_sequence_to_sequence_model(input_timesteps, input_features, output_timesteps):\n",
    "    # Input layer for the encoder\n",
    "    inputs = layers.Input(shape=(input_timesteps, input_features))\n",
    "\n",
    "    # Encoder LSTM: Outputs hidden states for each time step in the input sequence\n",
    "    encoder = layers.LSTM(64, activation='relu', return_state=True, return_sequences=False)\n",
    "    encoder_outputs, state_h, state_c = encoder(inputs)\n",
    "\n",
    "    # Decoder LSTM: We will generate output_timesteps (7) future time periods\n",
    "    decoder_input = layers.RepeatVector(output_timesteps)(encoder_outputs)\n",
    "\n",
    "    # Decoder LSTM generates the sequence of 7 time periods\n",
    "    decoder_lstm = layers.LSTM(64, activation='relu', return_sequences=True)\n",
    "    decoder_outputs = decoder_lstm(decoder_input, initial_state=[state_h, state_c])\n",
    "\n",
    "    # TimeDistributed layer to predict 32 features for each of the 7 time periods\n",
    "    time_distributed = layers.TimeDistributed(layers.Dense(input_features))\n",
    "    outputs = time_distributed(decoder_outputs)\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the input shape and output shape\n",
    "input_timesteps = 12  # 12 time periods (input sequence)\n",
    "input_features = 32   # 32 features per time period\n",
    "output_timesteps = 7  # Predict the next 7 time periods\n",
    "\n",
    "# Create the model\n",
    "model = create_sequence_to_sequence_model(input_timesteps, input_features, output_timesteps)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650489b2-d201-4ebf-a120-61bc3c854e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 12, 32)]             0         []                            \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)               [(None, 64),                 24832     ['input_3[0][0]']             \n",
      "                              (None, 64),                                                         \n",
      "                              (None, 64)]                                                         \n",
      "                                                                                                  \n",
      " repeat_vector_2 (RepeatVec  (None, 7, 64)                0         ['lstm_4[0][1]']              \n",
      " tor)                                                                                             \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)               (None, 7, 64)                33024     ['repeat_vector_2[0][0]',     \n",
      "                                                                     'lstm_4[0][1]',              \n",
      "                                                                     'lstm_4[0][2]']              \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 7, 32)                2080      ['lstm_5[0][0]']              \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 59936 (234.12 KB)\n",
      "Trainable params: 59936 (234.12 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model\n",
    "def create_sequence_to_sequence_model(input_timesteps, input_features, output_timesteps):\n",
    "    # Input layer for the encoder\n",
    "    inputs = layers.Input(shape=(input_timesteps, input_features))\n",
    "\n",
    "    # Encoder LSTM: Outputs the final hidden state (return_sequences=False)\n",
    "    encoder = layers.LSTM(64, activation='relu', return_state=True, return_sequences=False)\n",
    "    encoder_outputs, state_h, state_c = encoder(inputs)\n",
    "\n",
    "    # RepeatVector: Repeat the encoder's final hidden state for `output_timesteps` (7)\n",
    "    decoder_input = layers.RepeatVector(output_timesteps)(state_h)\n",
    "\n",
    "    # Decoder LSTM: Uses the repeated encoder final state and generates output_timesteps future time periods\n",
    "    decoder_lstm = layers.LSTM(64, activation='relu', return_sequences=True)\n",
    "    decoder_outputs = decoder_lstm(decoder_input, initial_state=[state_h, state_c])\n",
    "\n",
    "    # TimeDistributed layer to predict 32 features for each of the 7 time periods\n",
    "    time_distributed = layers.TimeDistributed(layers.Dense(input_features))\n",
    "    outputs = time_distributed(decoder_outputs)\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Model(inputs, outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the input shape and output shape\n",
    "input_timesteps = 12  # 12 time periods (input sequence)\n",
    "input_features = 32   # 32 features per time period\n",
    "output_timesteps = 7  # Predict the next 7 time periods\n",
    "\n",
    "# Create the model\n",
    "model = create_sequence_to_sequence_model(input_timesteps, input_features, output_timesteps)\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83816e-967d-4366-8359-ae5a2454aafd",
   "metadata": {},
   "source": [
    "I think all the three methods are producing the same code, and I think this is correct. We may still need to see if this is correct, or if this needs to be redefned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1c055-555c-453e-a4e6-759685198796",
   "metadata": {},
   "source": [
    "Redefine the code later with correct tf specifications, and I think it should be good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f0892-a61d-4252-a0f4-4407bf9320f6",
   "metadata": {},
   "source": [
    "Also, note that we are using an input size of 12, 32, in reality we have 13, 32 as the input. Adjust for this as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfp_for_TN)",
   "language": "python",
   "name": "tfp_for_tn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
